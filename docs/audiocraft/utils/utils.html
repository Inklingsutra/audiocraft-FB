<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>audiocraft.utils.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.utils.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from concurrent.futures import ProcessPoolExecutor
from functools import wraps
import hashlib
import logging
import typing as tp

import flashy
import flashy.distrib
import omegaconf
import torch
from torch.nn.utils.rnn import pad_sequence


logger = logging.getLogger(__name__)


def dict_from_config(cfg: omegaconf.DictConfig) -&gt; dict:
    &#34;&#34;&#34;Convenience function to map an omegaconf configuration to a dictionary.

    Args:
        cfg (omegaconf.DictConfig): Original configuration to map to dict.
    Returns:
        dict: Config as dictionary object.
    &#34;&#34;&#34;
    dct = omegaconf.OmegaConf.to_container(cfg, resolve=True)
    assert isinstance(dct, dict)
    return dct


def random_subset(dataset, max_samples: int, seed: int = 42) -&gt; torch.utils.data.Subset:
    if max_samples &gt;= len(dataset):
        return dataset

    generator = torch.Generator().manual_seed(seed)
    perm = torch.randperm(len(dataset), generator=generator)
    return torch.utils.data.Subset(dataset, perm[:max_samples].tolist())


def get_loader(dataset, num_samples: tp.Optional[int], batch_size: int,
               num_workers: int, seed: int, **kwargs) -&gt; torch.utils.data.DataLoader:
    &#34;&#34;&#34;Convenience function to load dataset into a dataloader with optional subset sampling.

    Args:
        dataset: Dataset to load.
        num_samples (Optional[int]): Number of samples to limit subset size.
        batch_size (int): Batch size.
        num_workers (int): Number of workers for data loading.
        seed (int): Random seed.
    &#34;&#34;&#34;
    if num_samples is not None:
        dataset = random_subset(dataset, num_samples, seed)

    dataloader = flashy.distrib.loader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        **kwargs
    )
    return dataloader


def get_dataset_from_loader(dataloader):
    dataset = dataloader.dataset
    if isinstance(dataset, torch.utils.data.Subset):
        return dataset.dataset
    else:
        return dataset


def multinomial(input: torch.Tensor, num_samples: int, replacement=False, *, generator=None):
    &#34;&#34;&#34;torch.multinomial with arbitrary number of dimensions, and number of candidates on the last dimension.

    Args:
        input (torch.Tensor): The input tensor containing probabilities.
        num_samples (int): Number of samples to draw.
        replacement (bool): Whether to draw with replacement or not.
    Keywords args:
        generator (torch.Generator): A pseudorandom number generator for sampling.
    Returns:
        torch.Tensor: Last dimension contains num_samples indices
            sampled from the multinomial probability distribution
            located in the last dimension of tensor input.
    &#34;&#34;&#34;
    input_ = input.reshape(-1, input.shape[-1])
    output_ = torch.multinomial(input_, num_samples=num_samples, replacement=replacement, generator=generator)
    output = output_.reshape(*list(input.shape[:-1]), -1)
    return output


def sample_top_k(probs: torch.Tensor, k: int) -&gt; torch.Tensor:
    &#34;&#34;&#34;Sample next token from top K values along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        k (int): The k in “top-k”.
    Returns:
        torch.Tensor: Sampled tokens.
    &#34;&#34;&#34;
    top_k_value, _ = torch.topk(probs, k, dim=-1)
    min_value_top_k = top_k_value[..., [-1]]
    probs *= (probs &gt;= min_value_top_k).float()
    probs.div_(probs.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs, num_samples=1)
    return next_token


def sample_top_p(probs: torch.Tensor, p: float) -&gt; torch.Tensor:
    &#34;&#34;&#34;Sample next token from top P probabilities along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        p (int): The p in “top-p”.
    Returns:
        torch.Tensor: Sampled tokens.
    &#34;&#34;&#34;
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort &gt; p
    probs_sort *= (~mask).float()
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token


class DummyPoolExecutor:
    &#34;&#34;&#34;Dummy pool executor to use when we actually have only 1 worker.
    (e.g. instead of ProcessPoolExecutor).
    &#34;&#34;&#34;
    class DummyResult:
        def __init__(self, func, *args, **kwargs):
            self.func = func
            self.args = args
            self.kwargs = kwargs

        def result(self):
            return self.func(*self.args, **self.kwargs)

    def __init__(self, workers, mp_context=None):
        pass

    def submit(self, func, *args, **kwargs):
        return DummyPoolExecutor.DummyResult(func, *args, **kwargs)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        return


def get_pool_executor(num_workers: int, mp_context=None):
    return ProcessPoolExecutor(num_workers, mp_context) if num_workers &gt; 1 else DummyPoolExecutor(1)


def length_to_mask(lengths: torch.Tensor, max_len: tp.Optional[int] = None) -&gt; torch.Tensor:
    &#34;&#34;&#34;Utility function to convert a tensor of sequence lengths to a mask (useful when working on padded sequences).
    For example: [3, 5] =&gt; [[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]]

    Args:
        lengths (torch.Tensor): tensor with lengths
        max_len (int): can set the max length manually. Defaults to None.
    Returns:
        torch.Tensor: mask with 0s where there is pad tokens else 1s
    &#34;&#34;&#34;
    assert len(lengths.shape) == 1, &#34;Length shape should be 1 dimensional.&#34;
    final_length = lengths.max().item() if not max_len else max_len
    final_length = max(final_length, 1)  # if all seqs are of len zero we don&#39;t want a zero-size tensor
    return torch.arange(final_length)[None, :].to(lengths.device) &lt; lengths[:, None]


def hash_trick(word: str, vocab_size: int) -&gt; int:
    &#34;&#34;&#34;Hash trick to pair each word with an index

    Args:
        word (str): word we wish to convert to an index
        vocab_size (int): size of the vocabulary
    Returns:
        int: index of the word in the embedding LUT
    &#34;&#34;&#34;
    hash = int(hashlib.sha256(word.encode(&#34;utf-8&#34;)).hexdigest(), 16)
    return hash % vocab_size


def with_rank_rng(base_seed: int = 1234):
    &#34;&#34;&#34;Decorator for a function so that the function will use a Random Number Generator
    whose state depend on the GPU rank. The original RNG state is restored upon returning.

    Args:
        base_seed (int): Random seed.
    &#34;&#34;&#34;
    def _decorator(fun: tp.Callable):
        @wraps(fun)
        def _decorated(*args, **kwargs):
            state = torch.get_rng_state()
            seed = base_seed ^ flashy.distrib.rank()
            torch.manual_seed(seed)
            logger.debug(&#39;Rank dependent seed set to %d&#39;, seed)
            try:
                return fun(*args, **kwargs)
            finally:
                torch.set_rng_state(state)
                logger.debug(&#39;RNG state restored.&#39;)
        return _decorated
    return _decorator


def collate(tensors: tp.List[torch.Tensor], dim: int = 0) -&gt; tp.Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Get a list of tensors and collate them to a single tensor. according to the following logic:
    - `dim` specifies the time dimension which will be stacked and padded.
    - The output will contain 1 new dimension (dimension index 0) which will be the size of
    of the original list.

    Args:
        tensors (tp.List[torch.Tensor]): List of tensors to collate.
        dim (int): Dimension which will be stacked and padded.
    Returns:
        tp.Tuple[torch.Tensor, torch.Tensor]:
            torch.Tensor: Stacked and padded tensor. The output will contain 1 new dimension
                (dimension index 0) which will be the size of the original list.
            torch.Tensor: Tensor containing length of original tensor sizes (without padding).
    &#34;&#34;&#34;
    tensors = [x.transpose(0, dim) for x in tensors]
    lens = torch.LongTensor([len(x) for x in tensors])
    padded_tensors = pad_sequence(tensors)
    padded_tensors = padded_tensors.transpose(0, 1)
    padded_tensors = padded_tensors.transpose(1, dim + 1)
    return padded_tensors, lens</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="audiocraft.utils.utils.collate"><code class="name flex">
<span>def <span class="ident">collate</span></span>(<span>tensors: List[torch.Tensor], dim: int = 0) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Get a list of tensors and collate them to a single tensor. according to the following logic:
- <code>dim</code> specifies the time dimension which will be stacked and padded.
- The output will contain 1 new dimension (dimension index 0) which will be the size of
of the original list.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong> :&ensp;<code>tp.List[torch.Tensor]</code></dt>
<dd>List of tensors to collate.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension which will be stacked and padded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>tp.Tuple[torch.Tensor, torch.Tensor]:</dt>
<dt><code>
torch.Tensor</code></dt>
<dd>Stacked and padded tensor. The output will contain 1 new dimension
(dimension index 0) which will be the size of the original list.
torch.Tensor: Tensor containing length of original tensor sizes (without padding).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collate(tensors: tp.List[torch.Tensor], dim: int = 0) -&gt; tp.Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Get a list of tensors and collate them to a single tensor. according to the following logic:
    - `dim` specifies the time dimension which will be stacked and padded.
    - The output will contain 1 new dimension (dimension index 0) which will be the size of
    of the original list.

    Args:
        tensors (tp.List[torch.Tensor]): List of tensors to collate.
        dim (int): Dimension which will be stacked and padded.
    Returns:
        tp.Tuple[torch.Tensor, torch.Tensor]:
            torch.Tensor: Stacked and padded tensor. The output will contain 1 new dimension
                (dimension index 0) which will be the size of the original list.
            torch.Tensor: Tensor containing length of original tensor sizes (without padding).
    &#34;&#34;&#34;
    tensors = [x.transpose(0, dim) for x in tensors]
    lens = torch.LongTensor([len(x) for x in tensors])
    padded_tensors = pad_sequence(tensors)
    padded_tensors = padded_tensors.transpose(0, 1)
    padded_tensors = padded_tensors.transpose(1, dim + 1)
    return padded_tensors, lens</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.dict_from_config"><code class="name flex">
<span>def <span class="ident">dict_from_config</span></span>(<span>cfg: omegaconf.dictconfig.DictConfig) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Convenience function to map an omegaconf configuration to a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong> :&ensp;<code>omegaconf.DictConfig</code></dt>
<dd>Original configuration to map to dict.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Config as dictionary object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dict_from_config(cfg: omegaconf.DictConfig) -&gt; dict:
    &#34;&#34;&#34;Convenience function to map an omegaconf configuration to a dictionary.

    Args:
        cfg (omegaconf.DictConfig): Original configuration to map to dict.
    Returns:
        dict: Config as dictionary object.
    &#34;&#34;&#34;
    dct = omegaconf.OmegaConf.to_container(cfg, resolve=True)
    assert isinstance(dct, dict)
    return dct</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.get_dataset_from_loader"><code class="name flex">
<span>def <span class="ident">get_dataset_from_loader</span></span>(<span>dataloader)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_from_loader(dataloader):
    dataset = dataloader.dataset
    if isinstance(dataset, torch.utils.data.Subset):
        return dataset.dataset
    else:
        return dataset</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.get_loader"><code class="name flex">
<span>def <span class="ident">get_loader</span></span>(<span>dataset, num_samples: Optional[int], batch_size: int, num_workers: int, seed: int, **kwargs) ‑> torch.utils.data.dataloader.DataLoader</span>
</code></dt>
<dd>
<div class="desc"><p>Convenience function to load dataset into a dataloader with optional subset sampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>Dataset to load.</dd>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Number of samples to limit subset size.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of workers for data loading.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Random seed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_loader(dataset, num_samples: tp.Optional[int], batch_size: int,
               num_workers: int, seed: int, **kwargs) -&gt; torch.utils.data.DataLoader:
    &#34;&#34;&#34;Convenience function to load dataset into a dataloader with optional subset sampling.

    Args:
        dataset: Dataset to load.
        num_samples (Optional[int]): Number of samples to limit subset size.
        batch_size (int): Batch size.
        num_workers (int): Number of workers for data loading.
        seed (int): Random seed.
    &#34;&#34;&#34;
    if num_samples is not None:
        dataset = random_subset(dataset, num_samples, seed)

    dataloader = flashy.distrib.loader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        **kwargs
    )
    return dataloader</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.get_pool_executor"><code class="name flex">
<span>def <span class="ident">get_pool_executor</span></span>(<span>num_workers: int, mp_context=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pool_executor(num_workers: int, mp_context=None):
    return ProcessPoolExecutor(num_workers, mp_context) if num_workers &gt; 1 else DummyPoolExecutor(1)</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.hash_trick"><code class="name flex">
<span>def <span class="ident">hash_trick</span></span>(<span>word: str, vocab_size: int) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Hash trick to pair each word with an index</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>word</code></strong> :&ensp;<code>str</code></dt>
<dd>word we wish to convert to an index</dd>
<dt><strong><code>vocab_size</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the vocabulary</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>index of the word in the embedding LUT</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hash_trick(word: str, vocab_size: int) -&gt; int:
    &#34;&#34;&#34;Hash trick to pair each word with an index

    Args:
        word (str): word we wish to convert to an index
        vocab_size (int): size of the vocabulary
    Returns:
        int: index of the word in the embedding LUT
    &#34;&#34;&#34;
    hash = int(hashlib.sha256(word.encode(&#34;utf-8&#34;)).hexdigest(), 16)
    return hash % vocab_size</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.length_to_mask"><code class="name flex">
<span>def <span class="ident">length_to_mask</span></span>(<span>lengths: torch.Tensor, max_len: Optional[int] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Utility function to convert a tensor of sequence lengths to a mask (useful when working on padded sequences).
For example: [3, 5] =&gt; [[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>lengths</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>tensor with lengths</dd>
<dt><strong><code>max_len</code></strong> :&ensp;<code>int</code></dt>
<dd>can set the max length manually. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>mask with 0s where there is pad tokens else 1s</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def length_to_mask(lengths: torch.Tensor, max_len: tp.Optional[int] = None) -&gt; torch.Tensor:
    &#34;&#34;&#34;Utility function to convert a tensor of sequence lengths to a mask (useful when working on padded sequences).
    For example: [3, 5] =&gt; [[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]]

    Args:
        lengths (torch.Tensor): tensor with lengths
        max_len (int): can set the max length manually. Defaults to None.
    Returns:
        torch.Tensor: mask with 0s where there is pad tokens else 1s
    &#34;&#34;&#34;
    assert len(lengths.shape) == 1, &#34;Length shape should be 1 dimensional.&#34;
    final_length = lengths.max().item() if not max_len else max_len
    final_length = max(final_length, 1)  # if all seqs are of len zero we don&#39;t want a zero-size tensor
    return torch.arange(final_length)[None, :].to(lengths.device) &lt; lengths[:, None]</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.multinomial"><code class="name flex">
<span>def <span class="ident">multinomial</span></span>(<span>input: torch.Tensor, num_samples: int, replacement=False, *, generator=None)</span>
</code></dt>
<dd>
<div class="desc"><p>torch.multinomial with arbitrary number of dimensions, and number of candidates on the last dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input tensor containing probabilities.</dd>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples to draw.</dd>
<dt><strong><code>replacement</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to draw with replacement or not.</dd>
</dl>
<p>Keywords args:
generator (torch.Generator): A pseudorandom number generator for sampling.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Last dimension contains num_samples indices
sampled from the multinomial probability distribution
located in the last dimension of tensor input.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multinomial(input: torch.Tensor, num_samples: int, replacement=False, *, generator=None):
    &#34;&#34;&#34;torch.multinomial with arbitrary number of dimensions, and number of candidates on the last dimension.

    Args:
        input (torch.Tensor): The input tensor containing probabilities.
        num_samples (int): Number of samples to draw.
        replacement (bool): Whether to draw with replacement or not.
    Keywords args:
        generator (torch.Generator): A pseudorandom number generator for sampling.
    Returns:
        torch.Tensor: Last dimension contains num_samples indices
            sampled from the multinomial probability distribution
            located in the last dimension of tensor input.
    &#34;&#34;&#34;
    input_ = input.reshape(-1, input.shape[-1])
    output_ = torch.multinomial(input_, num_samples=num_samples, replacement=replacement, generator=generator)
    output = output_.reshape(*list(input.shape[:-1]), -1)
    return output</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.random_subset"><code class="name flex">
<span>def <span class="ident">random_subset</span></span>(<span>dataset, max_samples: int, seed: int = 42) ‑> torch.utils.data.dataset.Subset</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_subset(dataset, max_samples: int, seed: int = 42) -&gt; torch.utils.data.Subset:
    if max_samples &gt;= len(dataset):
        return dataset

    generator = torch.Generator().manual_seed(seed)
    perm = torch.randperm(len(dataset), generator=generator)
    return torch.utils.data.Subset(dataset, perm[:max_samples].tolist())</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.sample_top_k"><code class="name flex">
<span>def <span class="ident">sample_top_k</span></span>(<span>probs: torch.Tensor, k: int) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Sample next token from top K values along the last dimension of the input probs tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>probs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input probabilities with token candidates on the last dimension.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>The k in “top-k”.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Sampled tokens.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_top_k(probs: torch.Tensor, k: int) -&gt; torch.Tensor:
    &#34;&#34;&#34;Sample next token from top K values along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        k (int): The k in “top-k”.
    Returns:
        torch.Tensor: Sampled tokens.
    &#34;&#34;&#34;
    top_k_value, _ = torch.topk(probs, k, dim=-1)
    min_value_top_k = top_k_value[..., [-1]]
    probs *= (probs &gt;= min_value_top_k).float()
    probs.div_(probs.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs, num_samples=1)
    return next_token</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.sample_top_p"><code class="name flex">
<span>def <span class="ident">sample_top_p</span></span>(<span>probs: torch.Tensor, p: float) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Sample next token from top P probabilities along the last dimension of the input probs tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>probs</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input probabilities with token candidates on the last dimension.</dd>
<dt><strong><code>p</code></strong> :&ensp;<code>int</code></dt>
<dd>The p in “top-p”.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Sampled tokens.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_top_p(probs: torch.Tensor, p: float) -&gt; torch.Tensor:
    &#34;&#34;&#34;Sample next token from top P probabilities along the last dimension of the input probs tensor.

    Args:
        probs (torch.Tensor): Input probabilities with token candidates on the last dimension.
        p (int): The p in “top-p”.
    Returns:
        torch.Tensor: Sampled tokens.
    &#34;&#34;&#34;
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort &gt; p
    probs_sort *= (~mask).float()
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token</code></pre>
</details>
</dd>
<dt id="audiocraft.utils.utils.with_rank_rng"><code class="name flex">
<span>def <span class="ident">with_rank_rng</span></span>(<span>base_seed: int = 1234)</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator for a function so that the function will use a Random Number Generator
whose state depend on the GPU rank. The original RNG state is restored upon returning.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Random seed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_rank_rng(base_seed: int = 1234):
    &#34;&#34;&#34;Decorator for a function so that the function will use a Random Number Generator
    whose state depend on the GPU rank. The original RNG state is restored upon returning.

    Args:
        base_seed (int): Random seed.
    &#34;&#34;&#34;
    def _decorator(fun: tp.Callable):
        @wraps(fun)
        def _decorated(*args, **kwargs):
            state = torch.get_rng_state()
            seed = base_seed ^ flashy.distrib.rank()
            torch.manual_seed(seed)
            logger.debug(&#39;Rank dependent seed set to %d&#39;, seed)
            try:
                return fun(*args, **kwargs)
            finally:
                torch.set_rng_state(state)
                logger.debug(&#39;RNG state restored.&#39;)
        return _decorated
    return _decorator</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.utils.utils.DummyPoolExecutor"><code class="flex name class">
<span>class <span class="ident">DummyPoolExecutor</span></span>
<span>(</span><span>workers, mp_context=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dummy pool executor to use when we actually have only 1 worker.
(e.g. instead of ProcessPoolExecutor).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DummyPoolExecutor:
    &#34;&#34;&#34;Dummy pool executor to use when we actually have only 1 worker.
    (e.g. instead of ProcessPoolExecutor).
    &#34;&#34;&#34;
    class DummyResult:
        def __init__(self, func, *args, **kwargs):
            self.func = func
            self.args = args
            self.kwargs = kwargs

        def result(self):
            return self.func(*self.args, **self.kwargs)

    def __init__(self, workers, mp_context=None):
        pass

    def submit(self, func, *args, **kwargs):
        return DummyPoolExecutor.DummyResult(func, *args, **kwargs)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        return</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.utils.utils.DummyPoolExecutor.DummyResult"><code class="name">var <span class="ident">DummyResult</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.utils.utils.DummyPoolExecutor.submit"><code class="name flex">
<span>def <span class="ident">submit</span></span>(<span>self, func, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def submit(self, func, *args, **kwargs):
    return DummyPoolExecutor.DummyResult(func, *args, **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.utils" href="index.html">audiocraft.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="audiocraft.utils.utils.collate" href="#audiocraft.utils.utils.collate">collate</a></code></li>
<li><code><a title="audiocraft.utils.utils.dict_from_config" href="#audiocraft.utils.utils.dict_from_config">dict_from_config</a></code></li>
<li><code><a title="audiocraft.utils.utils.get_dataset_from_loader" href="#audiocraft.utils.utils.get_dataset_from_loader">get_dataset_from_loader</a></code></li>
<li><code><a title="audiocraft.utils.utils.get_loader" href="#audiocraft.utils.utils.get_loader">get_loader</a></code></li>
<li><code><a title="audiocraft.utils.utils.get_pool_executor" href="#audiocraft.utils.utils.get_pool_executor">get_pool_executor</a></code></li>
<li><code><a title="audiocraft.utils.utils.hash_trick" href="#audiocraft.utils.utils.hash_trick">hash_trick</a></code></li>
<li><code><a title="audiocraft.utils.utils.length_to_mask" href="#audiocraft.utils.utils.length_to_mask">length_to_mask</a></code></li>
<li><code><a title="audiocraft.utils.utils.multinomial" href="#audiocraft.utils.utils.multinomial">multinomial</a></code></li>
<li><code><a title="audiocraft.utils.utils.random_subset" href="#audiocraft.utils.utils.random_subset">random_subset</a></code></li>
<li><code><a title="audiocraft.utils.utils.sample_top_k" href="#audiocraft.utils.utils.sample_top_k">sample_top_k</a></code></li>
<li><code><a title="audiocraft.utils.utils.sample_top_p" href="#audiocraft.utils.utils.sample_top_p">sample_top_p</a></code></li>
<li><code><a title="audiocraft.utils.utils.with_rank_rng" href="#audiocraft.utils.utils.with_rank_rng">with_rank_rng</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.utils.utils.DummyPoolExecutor" href="#audiocraft.utils.utils.DummyPoolExecutor">DummyPoolExecutor</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.utils.utils.DummyPoolExecutor.DummyResult" href="#audiocraft.utils.utils.DummyPoolExecutor.DummyResult">DummyResult</a></code></li>
<li><code><a title="audiocraft.utils.utils.DummyPoolExecutor.submit" href="#audiocraft.utils.utils.DummyPoolExecutor.submit">submit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>